{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f514891-a83e-46e7-b208-c949e77ad29a",
   "metadata": {},
   "source": [
    "# immoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19b280",
   "metadata": {},
   "source": [
    "## Presentation du Projet\n",
    "\n",
    "Ce projet à pour but d'exploiter les données Open source de l'immobillier francais à travers diverses applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70dcc1",
   "metadata": {},
   "source": [
    "## Source de données\n",
    "Notre source de données provient de Kaggle, site réputé, qui permet de récupérer et partager des sets de données mais aussi de pouvoir avoir une note de fiabilité et d'usabilité pour chaque dataset, le set *\"immobilier france\"* étant basé sur des sources officielles comme le *DVF+*, *IRCOM*, *la banque de france* et le *LOVAC* il obtient donc une note de *100% en crédibilité* et *100% en usabilité* grâce à un dataset documenté.\n",
    "\n",
    "\n",
    "Nous pouvons donc récupérer notre première [source](https://www.kaggle.com/datasets/benoitfavier/immobilier-france?select=transactions.npz) en format ```.npz```, format utilisé pour stocker des **arrays numpy**. Ce fichier comprend l'ensemble des transactions immobilières depuis 2014, ce sera le fichier central dans notre projet. Pour pouvoir lire ces données et les charger dans un **dataframe pandas** on peut utiliser le snippet de code fourni avec le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47660698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file = \"../sources/transactions.npz\"\n",
    "arrays = dict(np.load(file))\n",
    "data = {k: [s.decode(\"utf-8\") for s in v.tobytes().split(b\"\\x00\")] if v.dtype == np.uint8 else v for k, v in arrays.items()}\n",
    "original_transactions = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1664a64",
   "metadata": {},
   "source": [
    "## Exploration de données\n",
    "\n",
    "Regardons la structure de notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2319b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8318280 entries, 0 to 8318279\n",
      "Data columns (total 20 columns):\n",
      " #   Column                      Dtype         \n",
      "---  ------                      -----         \n",
      " 0   id_transaction              int32         \n",
      " 1   date_transaction            datetime64[ns]\n",
      " 2   prix                        float64       \n",
      " 3   departement                 object        \n",
      " 4   id_ville                    int32         \n",
      " 5   ville                       object        \n",
      " 6   code_postal                 int32         \n",
      " 7   adresse                     object        \n",
      " 8   type_batiment               object        \n",
      " 9   vefa                        bool          \n",
      " 10  n_pieces                    int32         \n",
      " 11  surface_habitable           int32         \n",
      " 12  id_parcelle_cadastre        object        \n",
      " 13  latitude                    float64       \n",
      " 14  longitude                   float64       \n",
      " 15  surface_dependances         object        \n",
      " 16  surface_locaux_industriels  object        \n",
      " 17  surface_terrains_agricoles  object        \n",
      " 18  surface_terrains_sols       object        \n",
      " 19  surface_terrains_nature     object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(3), int32(5), object(10)\n",
      "memory usage: 4.7 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(original_transactions.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd34b2",
   "metadata": {},
   "source": [
    "D'après la methode ```.info()``` nous avons un total de **8 318 280 lignes** et **20  colonnes** pour un total de **4.7 GB** de mémoire.\n",
    "\n",
    "Cet quantité de données va nous permettre d'entrainer un modèle de *Machine Learning* de façon qualitative mais necessite un traitement approfondit afin de supprimer un maximum de valeurs abérantes et d'erreurs ainsi qu'un stockage partitioné permettant de gagner un maximum de temps et de performances sur nos requêtes.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173caf2b",
   "metadata": {},
   "source": [
    "## Nettoyage des données\n",
    "\n",
    "Malgré les sources qualitatives dont est composé notre set, quelques erreurs ou cas particuliers peuvent compromettre l'entrainement de notre futur modèle. Nous devons donc établir des règles impartiales afin de considérer notre futur modèle previsionnel comme représentant la réalité. \n",
    "\n",
    "Pour commencer nous allons verifier qu'il n'y ai pas de valeur null ou NaN dans notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8224cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_transaction                0\n",
      "date_transaction              0\n",
      "prix                          0\n",
      "departement                   0\n",
      "id_ville                      0\n",
      "ville                         0\n",
      "code_postal                   0\n",
      "adresse                       0\n",
      "type_batiment                 0\n",
      "vefa                          0\n",
      "n_pieces                      0\n",
      "surface_habitable             0\n",
      "id_parcelle_cadastre          0\n",
      "latitude                      0\n",
      "longitude                     0\n",
      "surface_dependances           0\n",
      "surface_locaux_industriels    0\n",
      "surface_terrains_agricoles    0\n",
      "surface_terrains_sols         0\n",
      "surface_terrains_nature       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(original_transactions.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55da81a",
   "metadata": {},
   "source": [
    "Il n'y a donc aucune valeur null dans tout le dataset ✅\n",
    "\n",
    "\n",
    "Il nous faut maintenant verifier le typage des colonnes numériques afin d'être sûr qu'une string ne soit pas dans une des entrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d32f54dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8318280 entries, 0 to 8318279\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   prix               float64\n",
      " 1   surface_habitable  int32  \n",
      " 2   n_pieces           int32  \n",
      "dtypes: float64(1), int32(2)\n",
      "memory usage: 126.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(original_transactions[[\"prix\", \"surface_habitable\", \"n_pieces\"]].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59fffd",
   "metadata": {},
   "source": [
    "\n",
    "Pour cela nous devons donc établir un seuil pour le prix au m² à partir duquel la donnée est considérée absurde comparée aux autres entrées semblables.\\\n",
    "Nous avons donc décidé d'exclure toutes les entrées dont le prix/m² est superieur à la moyenne plus 3 fois l'écart-type ou inferieur à la médiane moins 3 fois l'écart-type des entrées de la même année et du même département, c'est à dire aux extrêmes de la répartition statistique.\n",
    "![schema ecart-type](../images/std.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "010738c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-06-24 16:57:24 - [INIT]: 202171 row suppressed ! That represent 2.49% of data'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def proc(df : pd.DataFrame):\n",
    "    # calculate the average\n",
    "    grouped_stats = df.groupby([pd.to_datetime(df['date_transaction']).dt.year, 'departement'])[\n",
    "        ['prix', 'surface_habitable']].apply(\n",
    "        lambda x: (x['prix'] / x['surface_habitable']).agg(['median', 'std'])).reset_index(drop=False)\n",
    "    # create a year column to join `grouped_stats` and initial dataframe\n",
    "    df['year'] = pd.to_datetime(df['date_transaction']).dt.year\n",
    "    # merge `grouped_stats` and initial dataframe\n",
    "    to_clean_df = pd.merge(df, grouped_stats, left_on=['year', 'departement'],\n",
    "                           right_on=['date_transaction', 'departement'], suffixes=('', '_stats'))\n",
    "    # remove absurd values where price/m² is 3 times more or less than the standard deviation\n",
    "    filtered_df = to_clean_df[\n",
    "        ((to_clean_df['prix'] / to_clean_df['surface_habitable']) < to_clean_df['median'] + 2 * to_clean_df['std']) & ((to_clean_df['prix'] / to_clean_df['surface_habitable']) > to_clean_df['median'] - 2 * to_clean_df['std'])]\n",
    "    # drop temp. column\n",
    "    filtered_df = filtered_df.drop(columns=['median', 'std', 'year'])\n",
    "    return filtered_df\n",
    "\n",
    "initial_len = len(original_transactions)\n",
    "\n",
    "original_transactions = proc(original_transactions)\n",
    "f\"{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))} - [INIT]: {initial_len - len(original_transactions)} row suppressed ! That represent {((initial_len - len(original_transactions)) / len(original_transactions)) * 100:.2f}% of data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c0b65",
   "metadata": {},
   "source": [
    "## Stockage"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "mportation des Données dans MySQL Cloud : Explications Détaillées\n",
    "L'importation des données dans MySQL Cloud se fait en plusieurs étapes. Voici une explication détaillée de chaque étape avec des exemples de code :\n",
    "\n",
    "1. Configuration de la Connexion à la Base de Données\n",
    "Pour connecter votre application à la base de données MySQL hébergée sur Google Cloud, nous utilisons SQLAlchemy. Assurez-vous d'avoir les informations de connexion stockées de manière sécurisée, par exemple, dans un fichier .env.\n",
    "\n",
    "Fichier .env :"
   ],
   "id": "d0688bb6d3a3d21a"
  },
  {
   "cell_type": "markdown",
   "id": "ae8d0e4d62351338",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
