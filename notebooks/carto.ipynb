{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f514891-a83e-46e7-b208-c949e77ad29a",
   "metadata": {},
   "source": [
    "# immoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19b280",
   "metadata": {},
   "source": [
    "## Presentation du Projet\n",
    "\n",
    "Ce projet à pour but d'exploiter les données Open source de l'immobillier francais à travers diverses applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70dcc1",
   "metadata": {},
   "source": [
    "## Source de données\n",
    "Notre source de données provient de Kaggle, site réputé, qui permet de récupérer et partager des sets de données mais aussi de pouvoir avoir une note de fiabilité et d'usabilité pour chaque dataset, le set *\"immobilier france\"* étant basé sur des sources officielles comme le *DVF+*, *IRCOM*, *la banque de france* et le *LOVAC* il obtient donc une note de *100% en crédibilité* et *100% en usabilité* grâce à un dataset documenté.\n",
    "\n",
    "\n",
    "Nous pouvons donc récupérer notre première [source](https://www.kaggle.com/datasets/benoitfavier/immobilier-france?select=transactions.npz) en format ```.npz```, format utilisé pour stocker des **arrays numpy**. Ce fichier comprend l'ensemble des transactions immobilières depuis 2014, ce sera le fichier central dans notre projet. Pour pouvoir lire ces données et les charger dans un **dataframe pandas** on peut utiliser le snippet de code fourni avec le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47660698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file = \"../sources/transactions.npz\"\n",
    "arrays = dict(np.load(file))\n",
    "data = {k: [s.decode(\"utf-8\") for s in v.tobytes().split(b\"\\x00\")] if v.dtype == np.uint8 else v for k, v in arrays.items()}\n",
    "original_transactions = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1664a64",
   "metadata": {},
   "source": [
    "## Exploration de données\n",
    "\n",
    "Regardons la structure de notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2319b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8318280 entries, 0 to 8318279\n",
      "Data columns (total 20 columns):\n",
      " #   Column                      Dtype         \n",
      "---  ------                      -----         \n",
      " 0   id_transaction              int32         \n",
      " 1   date_transaction            datetime64[ns]\n",
      " 2   prix                        float64       \n",
      " 3   departement                 object        \n",
      " 4   id_ville                    int32         \n",
      " 5   ville                       object        \n",
      " 6   code_postal                 int32         \n",
      " 7   adresse                     object        \n",
      " 8   type_batiment               object        \n",
      " 9   vefa                        bool          \n",
      " 10  n_pieces                    int32         \n",
      " 11  surface_habitable           int32         \n",
      " 12  id_parcelle_cadastre        object        \n",
      " 13  latitude                    float64       \n",
      " 14  longitude                   float64       \n",
      " 15  surface_dependances         object        \n",
      " 16  surface_locaux_industriels  object        \n",
      " 17  surface_terrains_agricoles  object        \n",
      " 18  surface_terrains_sols       object        \n",
      " 19  surface_terrains_nature     object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(3), int32(5), object(10)\n",
      "memory usage: 4.7 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(original_transactions.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd34b2",
   "metadata": {},
   "source": [
    "D'après la methode ```.info()``` nous avons un total de **8 318 280 lignes** et **20  colonnes** pour un total de **4.7 GB** de mémoire.\n",
    "\n",
    "Cet quantité de données va nous permettre d'entrainer un modèle de *Machine Learning* de façon qualitative mais necessite un traitement approfondit afin de supprimer un maximum de valeurs abérantes et d'erreurs ainsi qu'un stockage partitioné permettant de gagner un maximum de temps et de performances sur nos requêtes.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173caf2b",
   "metadata": {},
   "source": [
    "## Nettoyage des données\n",
    "\n",
    "Malgré les sources qualitatives dont est composé notre set, quelques erreurs ou cas particuliers peuvent compromettre l'entrainement de notre futur modèle. Nous devons donc établir des règles impartiales afin de considérer notre futur modèle previsionnel comme représentant la réalité. \n",
    "\n",
    "Pour commencer nous allons verifier qu'il n'y ai pas de valeur null ou NaN dans notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8224cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_transaction                0\n",
      "date_transaction              0\n",
      "prix                          0\n",
      "departement                   0\n",
      "id_ville                      0\n",
      "ville                         0\n",
      "code_postal                   0\n",
      "adresse                       0\n",
      "type_batiment                 0\n",
      "vefa                          0\n",
      "n_pieces                      0\n",
      "surface_habitable             0\n",
      "id_parcelle_cadastre          0\n",
      "latitude                      0\n",
      "longitude                     0\n",
      "surface_dependances           0\n",
      "surface_locaux_industriels    0\n",
      "surface_terrains_agricoles    0\n",
      "surface_terrains_sols         0\n",
      "surface_terrains_nature       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(original_transactions.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55da81a",
   "metadata": {},
   "source": [
    "Il n'y a donc aucune valeur null dans tout le dataset ✅\n",
    "\n",
    "\n",
    "Il nous faut maintenant verifier le typage des colonnes numériques afin d'être sûr qu'une string ne soit pas dans une des entrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d32f54dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8318280 entries, 0 to 8318279\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   prix               float64\n",
      " 1   surface_habitable  int32  \n",
      " 2   n_pieces           int32  \n",
      "dtypes: float64(1), int32(2)\n",
      "memory usage: 126.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(original_transactions[[\"prix\", \"surface_habitable\", \"n_pieces\"]].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59fffd",
   "metadata": {},
   "source": [
    "\n",
    "Pour cela nous devons donc établir un seuil pour le prix au m² à partir duquel la donnée est considérée absurde comparée aux autres entrées semblables.\\\n",
    "Nous avons donc décidé d'exclure toutes les entrées dont le prix/m² est superieur à la moyenne plus 3 fois l'écart-type ou inferieur à la médiane moins 3 fois l'écart-type des entrées de la même année et du même département, c'est à dire aux extrêmes de la répartition statistique.\n",
    "![schema ecart-type](../images/std.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "010738c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-06-24 16:57:24 - [INIT]: 202171 row suppressed ! That represent 2.49% of data'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "def proc(df : pd.DataFrame):\n",
    "    # calculate the average\n",
    "    grouped_stats = df.groupby([pd.to_datetime(df['date_transaction']).dt.year, 'departement'])[\n",
    "        ['prix', 'surface_habitable']].apply(\n",
    "        lambda x: (x['prix'] / x['surface_habitable']).agg(['median', 'std'])).reset_index(drop=False)\n",
    "    # create a year column to join `grouped_stats` and initial dataframe\n",
    "    df['year'] = pd.to_datetime(df['date_transaction']).dt.year\n",
    "    # merge `grouped_stats` and initial dataframe\n",
    "    to_clean_df = pd.merge(df, grouped_stats, left_on=['year', 'departement'],\n",
    "                           right_on=['date_transaction', 'departement'], suffixes=('', '_stats'))\n",
    "    # remove absurd values where price/m² is 3 times more or less than the standard deviation\n",
    "    filtered_df = to_clean_df[\n",
    "        ((to_clean_df['prix'] / to_clean_df['surface_habitable']) < to_clean_df['median'] + 2 * to_clean_df['std']) & ((to_clean_df['prix'] / to_clean_df['surface_habitable']) > to_clean_df['median'] - 2 * to_clean_df['std'])]\n",
    "    # drop temp. column\n",
    "    filtered_df = filtered_df.drop(columns=['median', 'std', 'year'])\n",
    "    return filtered_df\n",
    "\n",
    "initial_len = len(original_transactions)\n",
    "\n",
    "original_transactions = proc(original_transactions)\n",
    "f\"{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))} - [INIT]: {initial_len - len(original_transactions)} row suppressed ! That represent {((initial_len - len(original_transactions)) / len(original_transactions)) * 100:.2f}% of data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c0b65",
   "metadata": {},
   "source": [
    "## Stockage\n",
    "\n",
    "### Importation des Données dans MySQL Cloud\n",
    "L'importation des données dans MySQL Cloud se fait en plusieurs étapes. Voici une explication détaillée de chaque étape avec des exemples de code :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a73b2d",
   "metadata": {},
   "source": [
    "### 1. Création d'une instance MySQL sur Google Cloud\n",
    "- Connectez-vous à Google Cloud Console.\n",
    "- Accédez à la section SQL dans le menu de navigation.\n",
    "- Cliquez sur Créer une instance et choisissez MySQL.\n",
    "- Suivez les étapes pour configurer votre instance, y compris le choix de la version, les paramètres de l'instance, la région, la zone et le type de machine.\n",
    "- Notez le nom de l'instance, le nom d'utilisateur (par défaut root) et le mot de passe que vous définissez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b2c42",
   "metadata": {},
   "source": [
    "### 2. Configuration de l'accès réseau\n",
    "- Accédez à votre instance MySQL sur Google Cloud.\n",
    "- Sous la section Connexion au réseau, ajoutez votre adresse IP publique pour autoriser l'accès à distance.\n",
    "- Vous pouvez également ajouter une plage d'adresses IP pour permettre l'accès depuis plusieurs emplacements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c9e82",
   "metadata": {},
   "source": [
    "### 3. Configuration de la connexion à la base de données dans Python\n",
    "Assurez-vous que les informations de connexion sont stockées dans un fichier .env pour des raisons de sécurité.\n",
    "\n",
    "Fichier .env :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb0e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST=your-instance-public-ip\n",
    "DB_USER=your-db-username\n",
    "DB_PASSWORD=your-db-password\n",
    "DB_NAME=your-db-name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f93b9d0",
   "metadata": {},
   "source": [
    "Utilisez SQLAlchemy pour vous connecter à la base de données MySQL :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5e8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Connexion à la base de données\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{db_user}:{db_password}@{db_host}/{db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2344d0d",
   "metadata": {},
   "source": [
    "### 4. Importation des données dans MySQL\n",
    "Utilisez Pandas pour lire et nettoyer les données. Utilisez SQLAlchemy pour importer les données nettoyées dans la base de données MySQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d738ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lire le fichier de données\n",
    "file = \"./data/transactions.npz\"\n",
    "arrays = dict(np.load(file))\n",
    "data = {k: [s.decode(\"utf-8\") for s in v.tobytes().split(b\"\\x00\")] if v.dtype == np.uint8 else v for k, v in arrays.items()}\n",
    "df_transactions = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Nettoyer les données comme montré précédemment\n",
    "df_cleaned = proc(df_transactions)\n",
    "\n",
    "# Importer les données dans MySQL\n",
    "df_cleaned.to_sql('transactions', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c0b65",
   "metadata": {},
   "source": [
    "Cette configuration vous permet de gérer vos données efficacement et de les stocker dans une base de données MySQL sur Google Cloud pour un accès et une manipulation facilités."
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problèmes et Solutions en Machine Learning\n",
    "Dans ce notebook, nous allons charger et afficher le contenu d'un fichier JSON qui décrit le modèle de machine learning, les problèmes rencontrés et les solutions appliquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Charger les données JSON\n",
    "with open('data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Afficher les données JSON\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle\n",
    "Le modèle de machine learning utilisé est :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear Regression'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher le modèle utilisé\n",
    "data['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problèmes et Solutions\n",
    "### Problèmes rencontrés et solutions appliquées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Valeurs de Prédiction Constantes**\n",
       "  - Description : Les prédictions étaient constantes en raison d'une gestion incorrecte des caractéristiques temporelles.\n",
       "  - Solution : Ajout de l'année et du mois comme caractéristiques numériques continues.\n",
       "- **Temps d'Exécution Long**\n",
       "  - Description : Le script prenait trop de temps à s'exécuter.\n",
       "  - Solution : Ajout de la journalisation pour diagnostiquer le temps pris et optimisation du code."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les problèmes et solutions\n",
    "issues = data['issues']\n",
    "output = []\n",
    "for issue in issues:\n",
    "    output.append(f\"- **{issue['name']}**\")\n",
    "    output.append(f\"  - Description : {issue['description']}\")\n",
    "    output.append(f\"  - Solution : {issue['solution']}\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown('\\n'.join(output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métriques du Modèle\n",
    "Les métriques de performance du modèle sont les suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Score d'Entraînement** : 0.85\n",
       "- **Score de Test** : 0.80\n",
       "- **Erreur Quadratique Moyenne** : 5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les métriques du modèle\n",
    "metrics = data['metrics']\n",
    "output = []\n",
    "output.append(f\"- **Score d'Entraînement** : {metrics['train_score']}\")\n",
    "output.append(f\"- **Score de Test** : {metrics['test_score']}\")\n",
    "output.append(f\"- **Erreur Quadratique Moyenne** : {metrics['mean_squared_error']}\")\n",
    "\n",
    "display(Markdown('\\n'.join(output)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
